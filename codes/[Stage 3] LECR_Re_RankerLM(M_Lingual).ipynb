{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uUBoVyWVb0V"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!pip install transformers\n",
        "!pip install sentencepiece # for auto-tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUHd0S5vV0Av"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.swa_utils as swa\n",
        "import tokenizers, transformers\n",
        "import os, sys, gc, time, random, warnings, math\n",
        "\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, DataCollatorWithPadding\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from tqdm.auto import tqdm\n",
        "from glob import glob\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%env TOKENIZERS_PARALLELISM=false\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHxN24gAV4aP"
      },
      "outputs": [],
      "source": [
        "# WandB Login => Copy API Key\n",
        "secret_value_0 = '8d7716caaaa5afb56e1d02ef5837cabbffe48b41'\n",
        "!wandb login $secret_value_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv_OcKqkV7K2"
      },
      "outputs": [],
      "source": [
        "\"\"\" Train Configuration Class \"\"\"\n",
        "\n",
        "class CFG1:\n",
        "    wandb, train, competition, seed, cfg_name = True, True, 'LECR', 42, 'CFG1'\n",
        "    device, gpu_id = torch.device('cuda' if torch.cuda.is_available() else 'cpu'), 0\n",
        "    reranker = 'paraphrase-multilingual-mpnet-base-v2-exp_fold0_epochs10'\n",
        "    reranker_tokenizer = AutoTokenizer.from_pretrained(reranker + '/tokenizer')\n",
        "    pooling = 'attention' # options: attention, mean, weightedlayer, concat\n",
        "    max_len = 256\n",
        "    n_folds = 5 # cross val\n",
        "    loss_fn = 'BCE' # options: BCE, RMSE\n",
        "    epochs = 1\n",
        "    batch_size = 128 # 64 to 128\n",
        "    optimizer = 'AdamW' # options: SWA, AdamW\n",
        "    weight_decay = 1e-6\n",
        "    scheduler = 'cosine' # options: cosine, linear\n",
        "    num_cycles = 0.5\n",
        "    warmup_ratio = 0.1\n",
        "    batch_scheduler = True\n",
        "    encoder_lr = 5e-5\n",
        "    decoder_lr = 5e-5\n",
        "    min_lr = 1e-6\n",
        "    max_grad_norm = 1.0 # clip_grad_norm\n",
        "    gradient_checkpointing = True\n",
        "    num_workers = 0\n",
        "    amp_scaler = True\n",
        "    eps = 1e-6\n",
        "    betas = (0.9, 0.999)\n",
        "    llrd = True\n",
        "    layerwise_lr = 5e-5\n",
        "    layerwise_lr_decay = 0.9\n",
        "    layerwise_weight_decay = 0.01\n",
        "    layerwise_adam_epsilon = 1e-6\n",
        "    layerwise_use_bertadam = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF2kJF-BV-CE"
      },
      "outputs": [],
      "source": [
        "\"\"\" Configuration class to dict \"\"\"\n",
        "\n",
        "def class2dict(f):\n",
        "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vRHAeeaV_Um"
      },
      "outputs": [],
      "source": [
        "\"\"\" pytorch reproducibility functions \"\"\"\n",
        "\n",
        "qdef all_type_seed(CFG):\n",
        "    os.environ['PYTHONHASHSEED'] = str(CFG.seed) # python Seed \n",
        "    random.seed(CFG.seed) # random module Seed\n",
        "    np.random.seed(CFG.seed) # numpy module Seed\n",
        "    torch.manual_seed(CFG.seed)\n",
        "\n",
        "    torch.manual_seed(CFG.seed) # Pytorch CPU Random Seed Maker \n",
        "    torch.cuda.manual_seed(CFG.seed) # Pytorch GPU Random Seed Maker \n",
        "    torch.cuda.manual_seed_all(CFG.seed) # Pytorch Multi Core GPU Random Seed Maker \n",
        "\n",
        "    torch.backends.cudnn.deterministic = True \n",
        "    torch.backends.cudnn.benchmark = False \n",
        "    torch.backends.cudnn.enabled = False\n",
        "    \n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() %2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "    \n",
        "all_type_seed(CFG1)    \n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9gGBqZ7YW_z"
      },
      "outputs": [],
      "source": [
        "\"\"\" Load Data & Preprocess Function \"\"\"\n",
        "\n",
        "correlations = pd.read_csv('paraphrase-multilingual-mpnet-base-v2_kfold_0.csv')\n",
        "dataset = f'CFG1_train.csv'\n",
        "def load_data(dataset):\n",
        "#     # Merge All DataSet\n",
        "#     df= pd.DataFrame(columns=['topics_ids', 'topics_language', 'content_ids',\n",
        "#                               'content_language','title1', 'title2', 'target'])\n",
        "#     # drop duplicate row\n",
        "#     for dataset in dataset_list[0:2]:\n",
        "#         dataset = pd.read_csv(dataset)\n",
        "#         df = pd.concat([df,dataset], sort=True)\n",
        "#     df.drop_duplicates(['topics_ids', 'topics_language', 'content_ids',\n",
        "#                         'content_language','title1', 'title2', 'target'], inplace=True)\n",
        "    df = pd.read_csv(dataset)\n",
        "    \n",
        "    # fill NaNa\n",
        "    df['title1'].fillna(\"\", inplace=True)\n",
        "    df['title2'].fillna(\"\", inplace=True)\n",
        "    \n",
        "    # Merge topic.title & content.title\n",
        "    df['text'] = df['title1'] + '[SEP]' + df['title2']\n",
        "    df['text'].fillna(\"\", inplace=True)\n",
        "\n",
        "    \n",
        "    # preprocess\n",
        "    df['target'] = df['target'].astype('float')\n",
        "    df.reset_index(drop = True, inplace = True)\n",
        "    print(' ')\n",
        "    print('-' * 50)\n",
        "    print(f\"Newdataset.shape: {df.shape}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgMxzXAuYkfo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model Class for Re-Ranker Model(Sentence Transformers)\n",
        "\"\"\"\n",
        "\n",
        "class NewDataset(Dataset):\n",
        "    def __init__(self, df, CFG):\n",
        "        super().__init__()\n",
        "        self.text = df['text'].values\n",
        "        self.labels = df['target'].values\n",
        "        self.cfg = CFG\n",
        "        \n",
        "    def tokenizing(self, text_data):\n",
        "        inputs = self.cfg.reranker_tokenizer.encode_plus(text_data, \n",
        "                                                         return_tensors=None, # if true, tf.tensor, pt.tensor, numpy\n",
        "                                                         add_special_tokens=True,\n",
        "                                                         truncation=True,\n",
        "                                                         padding='max_length',\n",
        "                                                         max_length=self.cfg.max_len)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
        "        \n",
        "        return inputs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizing(self.text[idx]) # tokenizing\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return inputs, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NqyzTzEYlB-"
      },
      "outputs": [],
      "source": [
        "\"\"\" Pooling Functions \"\"\"\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(nn.Linear(in_dim, in_dim),\n",
        "                                       nn.LayerNorm(in_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Linear(in_dim, 1),)\n",
        "\n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        w = self.attention(last_hidden_state).float()\n",
        "        w[attention_mask==0]=float('-inf')\n",
        "        w = torch.softmax(w,1)\n",
        "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
        "        return attention_embeddings\n",
        "\n",
        "# Mean Pooling\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanPooling, self).__init__()\n",
        "        \n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "        return mean_embeddings\n",
        "\n",
        "# WeightedLayer Pooling\n",
        "class WeightedLayerPooling(nn.Module):\n",
        "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
        "        super(WeightedLayerPooling, self).__init__()\n",
        "        self.layer_start = layer_start\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
        "            else nn.Parameter(\n",
        "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
        "            )\n",
        "\n",
        "    def forward(self, features):\n",
        "        ft_all_layers = features['all_layer_embeddings']\n",
        "        all_layer_embedding = torch.stack(ft_all_layers)\n",
        "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
        "\n",
        "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
        "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
        "\n",
        "        features.update({'token_embeddings': weighted_average})\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZSL_8rgYmm8"
      },
      "outputs": [],
      "source": [
        "\"\"\" Model classs for Re-Ranker Model \"\"\"\n",
        "\n",
        "class ReRankerLM(nn.Module):\n",
        "    def __init__(self, CFG):\n",
        "        super().__init__()\n",
        "        self.cfg = CFG\n",
        "        self.auto_cfg = AutoConfig.from_pretrained(CFG.reranker + '/config',\n",
        "                                                   output_hidden_states = True)\n",
        "        self.model = AutoModel.from_pretrained(CFG.reranker + '/model',\n",
        "                                               config = self.auto_cfg)\n",
        "        self.fc = nn.Linear(self.auto_cfg.hidden_size, 1)\n",
        "        self._init_weights(self.fc)\n",
        "        \n",
        "        # pooling\n",
        "        if self.cfg.pooling == 'attention':\n",
        "            self.pooling = AttentionPooling(self.auto_cfg.hidden_size)\n",
        "        elif self.cfg.pooling == 'mean':\n",
        "            self.pooling = MeanPooling()\n",
        "        elif self.cfg.pooling == 'weightedlayer':\n",
        "            self.pooling = WeightedLayerPooling(self.auto_cfg.num_hidden_layers, layer_weights = None)\n",
        "\n",
        "        # checkpointing\n",
        "        if self.cfg.gradient_checkpointing:\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "         \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        outputs = self.model(**inputs) # inputs from LECRDataset\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        embedding = self.pooling(last_hidden_state,\n",
        "                                 inputs['attention_mask'])\n",
        "        output = self.fc(embedding)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGmQmU71Y2_Y"
      },
      "outputs": [],
      "source": [
        "\"\"\" Loss, Metric Tracker, Collate Function  \"\"\"\n",
        "\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.eps = eps # If MSE == 0, We need eps\n",
        "\n",
        "    def forward(self, yhat, y):\n",
        "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
        "        return loss\n",
        "    \n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def collate(inputs):\n",
        "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = inputs[k][:,:mask_len]\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD9ccT38Y4kt"
      },
      "outputs": [],
      "source": [
        "\"\"\" Trainer Input Class \"\"\"\n",
        "\n",
        "class TrainInput():\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        self.df = load_data(dataset) # return dataset\n",
        "        self.save_parameter = f'(best_score) {self.cfg.reranker}_state_dict.pth'\n",
        "    \n",
        "    # LLRD \n",
        "    def get_optimizer_grouped_parameters(self, model, layerwise_lr, layerwise_weight_decay, layerwise_lr_decay):\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        # initialize lr for task specific layer\n",
        "        optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
        "                                         \"weight_decay\": 0.0,\n",
        "                                         \"lr\": layerwise_lr,\n",
        "                                        },]\n",
        "        # initialize lrs for every layer\n",
        "        layers = [model.model.embeddings] + list(model.model.encoder.layer)\n",
        "        layers.reverse()\n",
        "        lr = layerwise_lr\n",
        "        for layer in layers:\n",
        "            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                                              \"weight_decay\": layerwise_weight_decay,\n",
        "                                              \"lr\": lr,\n",
        "                                             },\n",
        "                                             {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                                              \"weight_decay\": 0.0,\n",
        "                                              \"lr\": lr,\n",
        "                                             },]\n",
        "            lr *= layerwise_lr_decay\n",
        "        return optimizer_grouped_parameters\n",
        "        \n",
        "    def make_batch(self, fold):\n",
        "        train = self.df[self.df['fold'] != fold]\n",
        "        valid = self.df[self.df['fold'] == fold]\n",
        "        valid_labels = valid['target'].values\n",
        "\n",
        "        # Custom Dataset\n",
        "        train_dataset = NewDataset(train, self.cfg)\n",
        "        valid_dataset = NewDataset(valid, self.cfg)\n",
        "        \n",
        "        # DataLoader\n",
        "        loader_train = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size = self.cfg.batch_size,\n",
        "            shuffle = True,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g,\n",
        "            num_workers = self.cfg.num_workers,\n",
        "            pin_memory = True,\n",
        "            drop_last = True,\n",
        "        )\n",
        "        \n",
        "        loader_valid = DataLoader(\n",
        "            valid_dataset,\n",
        "            batch_size = self.cfg.batch_size,\n",
        "            shuffle = False,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g,\n",
        "            num_workers = self.cfg.num_workers,\n",
        "            pin_memory = True,\n",
        "            drop_last = False,\n",
        "        )\n",
        "        \n",
        "        return loader_train, loader_valid, train, valid\n",
        "\n",
        "    def model_setting(self):\n",
        "        # model\n",
        "        # Re-Initialze Weights of Encoder \n",
        "        model = ReRankerLM(self.cfg)\n",
        "        model.load_state_dict(torch.load('stage3_multi-lingual-mpnet.pth'))\n",
        "        model.to(self.cfg.device)\n",
        "        \n",
        "        # Setting Loss_Function\n",
        "        if self.cfg.loss_fn == 'BCE':\n",
        "            criterion = nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            criterion = RMSELoss()\n",
        "            \n",
        "        # optimizer\n",
        "        grouped_optimizer_params = self.get_optimizer_grouped_parameters(\n",
        "            model, \n",
        "            self.cfg.layerwise_lr, \n",
        "            self.cfg.layerwise_weight_decay, \n",
        "            self.cfg.layerwise_lr_decay\n",
        "        )\n",
        "        optimizer = AdamW(\n",
        "            grouped_optimizer_params,\n",
        "            lr = self.cfg.layerwise_lr,\n",
        "            eps = self.cfg.layerwise_adam_epsilon,\n",
        "            correct_bias = not self.cfg.layerwise_use_bertadam)\n",
        "        return model, criterion, optimizer, self.save_parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpH89qp5Y-Gs"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extract Embedding, Calculate Competiton Metric\n",
        "\"\"\"\n",
        "def f2_score(y_true, y_pred):\n",
        "    y_true = y_true.apply(lambda x: set(x.split()))\n",
        "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
        "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
        "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
        "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
        "    return round(f2.mean(), 4)\n",
        "\n",
        "def get_best_threshold(x_val, val_predictions, correlations):\n",
        "    best_score = 0\n",
        "    best_threshold = None\n",
        "    for thres in np.arange(0.001, 0.1, 0.001):\n",
        "        # if instance bigger than threshold, return 1 \n",
        "        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n",
        "        x_val1 = x_val[x_val['predictions'] == 1]\n",
        "        x_val1 = x_val1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
        "        x_val1['content_ids'] = x_val1['content_ids'].apply(lambda x: ' '.join(x))\n",
        "        x_val1.columns = ['topic_id', 'predictions']\n",
        "        x_val0 = pd.Series(x_val['topics_ids'].unique())\n",
        "        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n",
        "        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n",
        "        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n",
        "        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n",
        "        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])        \n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_threshold = thres\n",
        "    return best_score, best_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNAQUq8gY_zo"
      },
      "outputs": [],
      "source": [
        "# Step 3.1 Train & Validation Function\n",
        "def train_fn(cfg, loader_train, loader_valid, model, criterion, optimizer, scheduler, valid):\n",
        "    # Train Stages\n",
        "    # torch.amp.gradscaler\n",
        "    if cfg.amp_scaler:\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
        "    score_list = [] # All Fold's average of mean F2-Score\n",
        "    losses = AverageMeter()\n",
        "    model.train()\n",
        "    for inputs, target in tqdm(loader_train):\n",
        "        inputs = collate(inputs)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(cfg.device) # train to gpu\n",
        "        target = target.to(cfg.device) # label to gpu\n",
        "        batch_size = target.size(0)\n",
        "        with torch.cuda.amp.autocast(enabled = True):\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(inputs)\n",
        "            loss = criterion(preds.view(-1), target)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), cfg.max_grad_norm)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "            \n",
        "    # Validation Stage\n",
        "    valid_losses = AverageMeter()\n",
        "    preds_list, target_list = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in tqdm(loader_valid):\n",
        "            inputs = collate(inputs)\n",
        "            for k, v in inputs.items():\n",
        "                inputs[k] = v.to(cfg.device)\n",
        "            target_list.append(target)\n",
        "            target = target.to(cfg.device)\n",
        "            batch_size = target.size(0)\n",
        "            preds = model(inputs)\n",
        "            valid_loss = criterion(preds.view(-1), target)\n",
        "            valid_losses.update(valid_loss.item(), batch_size)\n",
        "            preds_list.append(preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))            \n",
        "            \n",
        "    predictions = np.concatenate(preds_list, axis = 0)\n",
        "    f2_score, fold_thres = get_best_threshold(valid, predictions, correlations)\n",
        "    fold_score = np.mean(f2_score)\n",
        "    \n",
        "    return losses.avg, valid_losses.avg, fold_score, fold_thres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi_KvNv1ZBQN"
      },
      "outputs": [],
      "source": [
        "\"\"\" Train/Validation Loop \"\"\"\n",
        "\n",
        "\n",
        "cfg_list = [CFG1]\n",
        "for cfg in cfg_list:\n",
        "    # init wandb\n",
        "    wandb.init(project=\"[kaggle] LECR\", \n",
        "               name=cfg.reranker,\n",
        "               config=class2dict(cfg),\n",
        "               group=cfg.reranker,\n",
        "               job_type=\"train\",\n",
        "               entity = \"qcqced\")\n",
        "    wandb_config = wandb.config\n",
        "    print(f'========================= Re-Ranker Model :{cfg.reranker} =========================')\n",
        "    train_input = TrainInput(cfg) # init object\n",
        "    model, criterion, optimizer, save_parameter = train_input.model_setting()\n",
        "    val_score_max, best_thres = -np.inf, 0\n",
        "    for epoch in range(cfg.epochs):\n",
        "        fold_list = [i for i in range(cfg.n_folds)]\n",
        "        epoch_train_loss, epoch_valid_loss, reranker_score, epoch_thres = [], [], [], []\n",
        "        for fold in tqdm(fold_list[1:2]):\n",
        "            print(f'============== {fold}th Fold Train & Validation ==============')\n",
        "            loader_train, loader_valid, train, valid = train_input.make_batch(fold)\n",
        "            # scheduler\n",
        "            if cfg.scheduler == 'cosine':\n",
        "                scheduler = get_cosine_schedule_with_warmup(\n",
        "                    optimizer, \n",
        "                    num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs) * cfg.warmup_ratio,\n",
        "                    num_training_steps=int(len(train)/cfg.batch_size * cfg.epochs),\n",
        "                    num_cycles = cfg.num_cycles\n",
        "                )  \n",
        "            else:\n",
        "                scheduler = get_linear_schedule_with_warmup(\n",
        "                    optimizer,\n",
        "                    num_warmup_steps=int(len(train)/cfg.batch_size * cfg.epochs) * cfg.warmup_ratio,\n",
        "                    num_training_steps=int(len(train) /cfg.batch_size * cfg.epochs),\n",
        "                    num_cycles = cfg.num_cycles\n",
        "                )  \n",
        "\n",
        "            train_loss, valid_loss, fold_score, fold_thres = train_fn(\n",
        "                cfg,\n",
        "                loader_train,\n",
        "                loader_valid,\n",
        "                model,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                valid\n",
        "            )\n",
        "            epoch_train_loss.append(train_loss)\n",
        "            epoch_valid_loss.append(valid_loss)\n",
        "            reranker_score.append(fold_score)\n",
        "            epoch_thres.append(fold_thres)\n",
        "            wandb.log({'Fold Train Loss': train_loss,\n",
        "                       'Fold Valid Loss': valid_loss,\n",
        "                       'Fold Mean F2-Score': fold_score,\n",
        "                       'Fold Best Threshold': fold_thres,})\n",
        "\n",
        "            print(f'fold[{fold}/{fold_list[-1]}] Train Loss: {round(train_loss, 4)}') # Best Threshold Value\n",
        "            print(f'fold[{fold}/{fold_list[-1]}] Valid Loss: {round(valid_loss, 4)}') # Best Threshold Value\n",
        "            print(f'fold[{fold}/{fold_list[-1]}] Mean F2-Score: {round(fold_score, 4)}') # Best Threshold Value\n",
        "        \n",
        "        epoch_train_loss = np.mean(epoch_train_loss)\n",
        "        epoch_valid_loss = np.mean(epoch_valid_loss)\n",
        "        epoch_score = np.mean(reranker_score)\n",
        "        epoch_thres = np.mean(epoch_thres)\n",
        "        wandb.log({'Train Loss': epoch_train_loss,\n",
        "                   'Valid Loss': epoch_valid_loss,\n",
        "                   'Mean F2-Score': epoch_score,\n",
        "                   'Threshold': epoch_thres,})\n",
        "        \n",
        "        print(f'================= {epoch}th Train & Validation =================')\n",
        "        print(f'epoch[{epoch+1}/{cfg.epochs}] Train Loss: {round(epoch_train_loss, 4)}')\n",
        "        print(f'epoch[{epoch+1}/{cfg.epochs}] Valid Loss: {round(epoch_valid_loss, 4)}')\n",
        "        print(f'epoch[{epoch+1}/{cfg.epochs}] Mean F2-Score: {round(epoch_score, 4)}')\n",
        "        print(f'epoch[{epoch+1}/{cfg.epochs}] Threshold: {round(epoch_thres, 4)}')\n",
        "        \n",
        "        if val_score_max <= epoch_score:\n",
        "            print(f'[Update] Valid Score : ({val_score_max:.4f} => {epoch_score:.4f}) Save Parameter')\n",
        "            torch.save(model.state_dict(),\n",
        "                       'stage3_multi-lingual-mpnet.pth')\n",
        "            best_thres = epoch_thres\n",
        "            \n",
        "    wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JPKDV_UJayD0"
      },
      "source": [
        "[Train History]\n",
        "- 1th Epoch: 4, 3, 2, 1, 0\n",
        "- 2th Epoch: 2, 0, 3, 4, 1\n",
        "\n",
        "[Train Config History]\n",
        "- batch size: 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCp0RLbalfvn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
